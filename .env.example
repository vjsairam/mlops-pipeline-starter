# ==============================================================================
# Data Pipeline - Environment Configuration Template
# ==============================================================================
# SECURITY WARNING: This is a template file. Copy to .env and fill with actual values.
# NEVER commit .env file with real secrets to version control!
#
# Usage:
#   cp .env.example .env
#   Edit .env with your actual configuration
#   Ensure .env is in .gitignore (already configured)
# ==============================================================================

# Environment
ENVIRONMENT=development
DEBUG=false
LOG_LEVEL=INFO

# ==============================================================================
# Database Configuration
# ==============================================================================
POSTGRES_USER=pipeline_user
POSTGRES_PASSWORD=CHANGE_ME_STRONG_PASSWORD_HERE
POSTGRES_DB=pipeline
POSTGRES_PORT=5432
DATABASE_URL=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@localhost:${POSTGRES_PORT}/${POSTGRES_DB}

# ==============================================================================
# Redis Configuration
# ==============================================================================
REDIS_PASSWORD=CHANGE_ME_STRONG_REDIS_PASSWORD
REDIS_PORT=6379
REDIS_MAX_MEMORY=256mb
REDIS_URL=redis://:${REDIS_PASSWORD}@localhost:${REDIS_PORT}/0

# ==============================================================================
# Airflow Configuration
# ==============================================================================
# Generate Fernet key: python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
AIRFLOW_FERNET_KEY=CHANGE_ME_GENERATE_FERNET_KEY
# Generate secret key: openssl rand -hex 32
AIRFLOW_SECRET_KEY=CHANGE_ME_GENERATE_SECRET_KEY
AIRFLOW_USERNAME=admin
AIRFLOW_PASSWORD=CHANGE_ME_STRONG_AIRFLOW_PASSWORD
AIRFLOW_WEBSERVER_PORT=8080

# ==============================================================================
# Prometheus Configuration
# ==============================================================================
PROMETHEUS_PORT=9090
PROMETHEUS_RETENTION_TIME=30d

# ==============================================================================
# Grafana Configuration
# ==============================================================================
GRAFANA_ADMIN_USER=admin
GRAFANA_ADMIN_PASSWORD=CHANGE_ME_STRONG_GRAFANA_PASSWORD
# Generate secret: openssl rand -base64 32
GRAFANA_SECRET_KEY=CHANGE_ME_GENERATE_GRAFANA_SECRET
GRAFANA_PORT=3000
GRAFANA_ROOT_URL=http://localhost:3000
GRAFANA_PLUGINS=

# ==============================================================================
# API Configuration
# ==============================================================================
API_PORT=8000
API_WORKERS=4
API_HOST=0.0.0.0
MAX_REQUEST_SIZE=10485760  # 10MB

# ==============================================================================
# Security Configuration
# ==============================================================================
# JWT Configuration
# Generate: openssl rand -hex 32
JWT_SECRET_KEY=CHANGE_ME_GENERATE_JWT_SECRET
JWT_ALGORITHM=HS256
JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
JWT_REFRESH_TOKEN_EXPIRE_DAYS=7

# API Keys (comma-separated for multiple keys)
API_KEYS=CHANGE_ME_GENERATE_API_KEYS

# CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:8080
CORS_ALLOW_CREDENTIALS=true

# ==============================================================================
# Cloud Provider Configuration (Optional)
# ==============================================================================
# AWS
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=
AWS_SECRET_ACCESS_KEY=
S3_BUCKET_NAME=data-pipeline-storage

# GCP
GCP_PROJECT_ID=
GCP_REGION=us-central1
GCS_BUCKET_NAME=data-pipeline-storage
GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account.json

# Azure
AZURE_SUBSCRIPTION_ID=
AZURE_RESOURCE_GROUP=data-pipeline-rg
AZURE_STORAGE_ACCOUNT=datapipelinestorage
AZURE_CONTAINER_NAME=pipeline-data

# ==============================================================================
# Data Storage Configuration
# ==============================================================================
DATA_LAKE_PATH=s3://data-lake/pipeline
RAW_DATA_PATH=data/raw
PROCESSED_DATA_PATH=data/processed
ARCHIVE_DATA_PATH=data/archive

# ==============================================================================
# Monitoring & Alerting Configuration
# ==============================================================================
# Slack
SLACK_WEBHOOK_URL=https://hooks.slack.com/services/YOUR/WEBHOOK/URL
SLACK_CHANNEL=#data-pipeline-alerts

# PagerDuty
PAGERDUTY_API_KEY=
PAGERDUTY_SERVICE_ID=

# Email
EMAIL_SMTP_HOST=smtp.gmail.com
EMAIL_SMTP_PORT=587
EMAIL_SMTP_USER=
EMAIL_SMTP_PASSWORD=
EMAIL_FROM_ADDRESS=alerts@example.com
EMAIL_TO_ADDRESSES=admin@example.com

# Sentry (Error Tracking)
SENTRY_DSN=
SENTRY_ENVIRONMENT=${ENVIRONMENT}
SENTRY_TRACES_SAMPLE_RATE=0.1

# ==============================================================================
# Data Pipeline Configuration
# ==============================================================================
# Processing
MAX_WORKERS=4
BATCH_SIZE=1000
PROCESSING_TIMEOUT_SECONDS=300
RETRY_ATTEMPTS=3
RETRY_DELAY_SECONDS=60

# Data Quality
DATA_QUALITY_ENABLED=true
DATA_QUALITY_FAIL_ON_ERROR=true
DATA_QUALITY_REPORT_PATH=reports/data_quality

# ==============================================================================
# Cache Configuration
# ==============================================================================
CACHE_TTL_SECONDS=3600
CACHE_MAX_SIZE_MB=1024
CACHE_ENABLED=true

# ==============================================================================
# Rate Limiting
# ==============================================================================
RATE_LIMIT_ENABLED=true
RATE_LIMIT_PER_MINUTE=100
RATE_LIMIT_PER_HOUR=5000

# ==============================================================================
# Compliance & Audit
# ==============================================================================
AUDIT_LOGGING_ENABLED=true
DATA_RETENTION_DAYS=90
ARCHIVE_RETENTION_DAYS=365
GDPR_COMPLIANCE_MODE=false
PII_DETECTION_ENABLED=true

# ==============================================================================
# Performance & Scaling
# ==============================================================================
CONNECTION_POOL_SIZE=20
CONNECTION_POOL_MAX_OVERFLOW=10
QUERY_TIMEOUT_MS=30000
REQUEST_TIMEOUT_MS=5000

# ==============================================================================
# Kubernetes Configuration (Production)
# ==============================================================================
K8S_NAMESPACE=data-pipeline
K8S_CONTEXT=production-cluster
K8S_CONFIG_PATH=~/.kube/config

# ==============================================================================
# Secrets Management (Production)
# ==============================================================================
# Vault
VAULT_URL=http://localhost:8200
VAULT_TOKEN=
VAULT_SECRET_PATH=secret/data/pipeline

# AWS Secrets Manager
AWS_SECRETS_MANAGER_ENABLED=false
AWS_SECRETS_REGION=${AWS_REGION}

# ==============================================================================
# Feature Flags
# ==============================================================================
ENABLE_CANARY_DEPLOYMENT=false
ENABLE_BLUE_GREEN_DEPLOYMENT=false
ENABLE_CIRCUIT_BREAKER=true
ENABLE_REQUEST_VALIDATION=true
ENABLE_RESPONSE_COMPRESSION=true

# ==============================================================================
# Development/Testing
# ==============================================================================
# Set to true only in development
MOCK_EXTERNAL_SERVICES=false
SKIP_AUTH_IN_DEV=false
ENABLE_DEBUG_ENDPOINTS=false
